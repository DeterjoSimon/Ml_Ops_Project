/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch:   0%|                                            | 0/5 [00:08<?, ?it/s]
Traceback (most recent call last):
  File "src/models/train_model.py", line 50, in <module>
    outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py", line 1164, in forward
    return_dict=return_dict,
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py", line 825, in forward
    return_dict=return_dict,
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py", line 515, in forward
    output_attentions,
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py", line 436, in forward
    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/transformers/modeling_utils.py", line 1995, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py", line 447, in feed_forward_chunk
    intermediate_output = self.intermediate(attention_output)
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py", line 349, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/zhome/22/4/118839/Ml_Ops_Project/Venv/lib64/python3.6/site-packages/torch/nn/functional.py", line 1383, in gelu
    return torch._C._nn.gelu(input)
KeyboardInterrupt